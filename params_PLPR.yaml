###########################
### Experiment Settings ###
###########################
# Logging to 'stdout' (good for testing) or 'file' (good for reproduce)
log_type: stdout
# Level logging output ('DEBUG', 'INFO', etc.)
log_level: DEBUG
# Type of experiment to run
type: PLPR
# Seed for random number generator
random_seed: 123
# Number of repetitions of an experiment (50)
runs: 3
# Number of training episodes within an experiment (1000)
episodes: 1000
# Maximum number of steps in one episode
max_steps: 100
# Number of episodes between testing episodes (1)
test_interval: 10
# Number of episodes between policy evaluations during training (10)
policy_eval_interval: 25
# Number of tests to evaluate policy (100)
policy_eval_episodes: 1
# Factor to determine similarity (closeness) as percentage of map diagonal
policy_eval_factor: 0.3
# Starting confidence in eval policy
policy_eval_confidence: 1.0
# Loss of confidence per step to explore more towards end of evaluation
policy_eval_conf_delta: 0.02
# Loss of confidence per step to explore more towards end of evaluation
policy_eval_conf_stop: 0.2
# Amount of states which is used to test against other policies
policy_eval_states: 500
# Limit of overlap to take new policy into library
policy_similarity_limit: 0.7
# interval when the policy with the lowest policy probability is cut
# policy_eliminate_interval: 1000
# at which probability do we start not using a certain policy anymore (0.1)
policy_importance_limit: 0.1
# probability to reuse a policy from the library
policy_reuse_probability: 1.0
# probability to reuse a policy from the library
policy_reuse_probability_decay: 1.0
# similarity factor for a policy, decides if policy is added to library (delta)
policy_library_simimilarity: 0.25
# max policies for current task
task_library_size: 3
# temperature factor for action probabillities with softmax (0.01)
tau_action: 0.01
# temperature factor for policy probabillities with softmax (0.15) 1.5
tau_policy: 1.0
# temperature factor for policy probabillities with softmax (0.0025) 0.02
tau_policy_delta: 0.05
# to evaluate the current performance count steps for these
# starting positions
test_positions:
    - [1, 1]
    - [7, 1]
    - [1, 11]
    - [3, 17]
    - [9, 11]
    - [20, 5]
    - [12, 9]
    - [21, 19]
    - [12, 15]
    - [22, 3]
# describe tasks by name and goal position
tasks:
    - name: omega
      goal_pos: [20, 2]
# will we use pretrained policies
load_policies: True
# where are those policies
learned_policies:
    - name: omega1
      goal_pos: [15, 2]
      directory: /Users/rubenglatt/playground/CBPI/logs/1_2017-03-18_16-28_q_policyreuse2006/task_omega1/
    - name: omega2
      goal_pos: [3, 2]
      directory: /Users/rubenglatt/playground/CBPI/logs/1_2017-03-18_16-28_q_policyreuse2006/task_omega2/
    - name: omega3
      goal_pos: [3, 18]
      directory: /Users/rubenglatt/playground/CBPI/logs/1_2017-03-18_16-28_q_policyreuse2006/task_omega3/
    - name: omega4
      goal_pos: [20, 18]
      directory: /Users/rubenglatt/playground/CBPI/logs/1_2017-03-18_16-28_q_policyreuse2006/task_omega4/
    - name: omega5
      goal_pos: [20, 2]
      directory: /Users/rubenglatt/playground/CBPI/logs/1_2017-03-18_16-28_q_policyreuse2006/task_omega5/
############################
### Environment Settings ###
############################
grid: PolicyReuse2006
visual: False
#########################
### Learning Settings ###
#########################
epsilon: 1.0
epsilon_change: -0.0005
alpha: 0.05
gamma: 0.95
