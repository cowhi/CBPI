###########################
### Experiment Settings ###
###########################
# Logging to 'stdout' (good for testing) or 'file' (good for reproduce)
log_type: file
# Level logging output ('DEBUG', 'INFO', etc.)
log_level: INFO
# Type of experiment to run
type: PPR
# Seed for random number generator
random_seed: 123
# Number of repetitions of an experiment (50)
runs: 10
# Number of training episodes within an experiment (1000)
episodes: 1000
# Maximum number of steps in one episode
max_steps: 100
# Number of episodes between testing episodes (1)
test_interval: 10
# Number of episodes between policy evaluations during training (10)
policy_eval_interval: 25
# Number of tests to evaluate policy (1)
policy_eval_episodes: 1
# Factor to determine similarity (closeness) as percentage of map diagonal
policy_eval_factor: 0.3
# Starting confidence in eval policy
policy_eval_confidence: 1.0
# Loss of confidence per step to explore more towards end of evaluation
policy_eval_conf_delta: 0.02
# Loss of confidence per step to explore more towards end of evaluation
policy_eval_conf_stop: 0.2
# Amount of states which is used to test against other policies
policy_eval_states: 500
# Limit of overlap to take new policy into library
policy_similarity_limit: 0.25
# interval when the policy with the lowest policy probability is cut
# policy_eliminate_interval: 1000
# at which probability do we start not using a certain policy anymore (0.1)
policy_importance_limit: 0.1
# max policies for current task
task_library_size: 3
# temperature factor for action probabillities with softmax (0.01)
tau_action: 0.01
# temperature factor for policy probabillities with softmax (0.15)
tau_policy: 1.5
# temperature factor for policy probabillities with softmax (0.0025)
tau_policy_change: 0.02
# to evaluate the current performance count steps for these
# starting positions
test_positions:
    - [1, 1]
    - [7, 1]
    - [1, 11]
    - [3, 17]
    - [9, 11]
    - [20, 5]
    - [12, 9]
    - [21, 19]
    - [12, 15]
    - [22, 3]
# describe tasks by name and goal position
tasks:
    - name: omega1
      goal_pos: [18, 1]
    - name: omega2
      goal_pos: [3, 2]
    - name: omega3
      goal_pos: [3, 18]
    - name: omega4
      goal_pos: [20, 18]
    - name: omega5
      goal_pos: [14, 1]
    - name: omega6
      goal_pos: [2, 2]
    - name: omega8
      goal_pos: [21, 6]
    - name: omega9
      goal_pos: [20, 11]
    - name: omega10
      goal_pos: [1, 11]
    - name: omega11
      goal_pos: [8, 18]
    - name: omega12
      goal_pos: [15, 18]
    - name: omega13
      goal_pos: [22, 17]
    - name: omega14
      goal_pos: [8, 2]
    - name: omega15
      goal_pos: [1, 6]
    - name: omega16
      goal_pos: [14, 8]
    - name: omega17
      goal_pos: [9, 11]
    - name: omega18
      goal_pos: [10, 12]
    - name: omega19
      goal_pos: [12, 12]
    - name: omega20
      goal_pos: [13, 11]
    - name: omega21
      goal_pos: [20, 3]
    - name: omega22
      goal_pos: [22, 11]
    - name: omega23
      goal_pos: [14, 9]
    - name: omega24
      goal_pos: [9, 8]
    - name: omega25
      goal_pos: [12, 9]
    - name: omega26
      goal_pos: [2, 19]
    - name: omega27
      goal_pos: [11, 2]
    - name: omega28
      goal_pos: [20, 7]
    - name: omega29
      goal_pos: [20, 10]
    - name: omega30
      goal_pos: [3, 10]
    - name: omega31
      goal_pos: [3, 7]
    - name: omega32
      goal_pos: [15, 3]
    - name: omega33
      goal_pos: [15, 19]
    - name: omega34
      goal_pos: [10, 19]
    - name: omega35
      goal_pos: [5, 1]
    - name: omega36
      goal_pos: [19, 17]
    - name: omega37
      goal_pos: [13, 17]
    - name: omega38
      goal_pos: [10, 17]
    - name: omega39
      goal_pos: [3, 17]
    - name: omega40
      goal_pos: [3, 12]
    - name: omega41
      goal_pos: [3, 5]
    - name: omega42
      goal_pos: [9, 1]
    - name: omega43
      goal_pos: [15, 1]
    - name: omega44
      goal_pos: [6, 19]
    - name: omega45
      goal_pos: [19, 19]
    - name: omega46
      goal_pos: [2, 3]
    - name: omega47
      goal_pos: [21, 1]
    - name: omega48
      goal_pos: [21, 5]
    - name: omega49
      goal_pos: [14, 12]
    - name: omega50
      goal_pos: [2, 8]
    - name: omega7
      goal_pos: [20, 2]
# will we use pretrained policies
load_policies: False
# where are those policies
learned_policies:
    - name: omega1
      goal_pos: [15, 2]
      directory: /Users/rubenglatt/playground/CBPI/logs/1_2017-03-18_16-28_q_policyreuse2006/task_omega1/
    - name: omega2
      goal_pos: [3, 2]
      directory: /Users/rubenglatt/playground/CBPI/logs/1_2017-03-18_16-28_q_policyreuse2006/task_omega2/
    - name: omega3
      goal_pos: [3, 18]
      directory: /Users/rubenglatt/playground/CBPI/logs/1_2017-03-18_16-28_q_policyreuse2006/task_omega3/
    - name: omega4
      goal_pos: [20, 18]
      directory: /Users/rubenglatt/playground/CBPI/logs/1_2017-03-18_16-28_q_policyreuse2006/task_omega4/
    - name: omega5
      goal_pos: [20, 2]
      directory: /Users/rubenglatt/playground/CBPI/logs/1_2017-03-18_16-28_q_policyreuse2006/task_omega5/
############################
### Environment Settings ###
############################
grid: PolicyReuse2006
visual: False
#########################
### Learning Settings ###
#########################
epsilon: 1.0
epsilon_change: -0.0005
alpha: 0.05
gamma: 0.95
